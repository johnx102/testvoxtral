# Configuration RunPod Serverless - Guide Rapide

## ðŸš€ DÃ©ploiement depuis GitHub

### 1. Variables d'Environnement Requises

Dans la configuration de votre endpoint RunPod, ajoutez ces variables :

```bash
# OBLIGATOIRE - Pour tÃ©lÃ©charger les modÃ¨les
HF_TOKEN=hf_votre_token_huggingface

# Cache persistant (dÃ©jÃ  configurÃ© dans le Dockerfile)
HF_HOME=/workspace/.cache/huggingface
TORCH_HOME=/workspace/.cache/torch

# Configuration du modÃ¨le (optionnel, valeurs par dÃ©faut dans Dockerfile)
MODEL_ID=mistralai/Voxtral-Small-24B-2507
DIAR_MODEL=pyannote/speaker-diarization-3.1
SENTIMENT_MODEL=MoritzLaurer/mDeBERTa-v3-base-mnli-xnli
```

### 2. Configuration GitHub

Dans RunPod, configurez :
- **Repository:** `votre-username/votre-repo`
- **Branch:** `main` (ou votre branche)
- **Dockerfile Path:** `Dockerfile`

### 3. Build Automatique

RunPod va :
1. âœ… Cloner votre repo GitHub
2. âœ… DÃ©tecter `HF_TOKEN` dans les variables d'environnement
3. âœ… Builder l'image Docker
4. ðŸš€ **Automatiquement prÃ©-cacher les modÃ¨les** si `HF_TOKEN` est prÃ©sent
5. âœ… DÃ©ployer l'endpoint serverless

**Logs du build Ã  surveiller :**
```
[BUILD] Checking for HF_TOKEN to pre-cache models...
[BUILD] HF_TOKEN found - Pre-caching models...
[WARMUP] Starting model pre-caching...
[WARMUP] âœ“ Voxtral processor cached
[WARMUP] âœ“ Diarization model cached
[WARMUP] âœ“ Sentiment model cached
```

Si vous voyez ces messages, le prÃ©-cache fonctionne ! ðŸŽ‰

### 4. Configuration RecommandÃ©e

**GPU :**
- Type: RTX A6000 (48GB VRAM) ou supÃ©rieur
- RecommandÃ©: A100 (80GB) pour les meilleures performances

**Workers :**
- **Min Workers:** 1 (garde une instance warm pour Ã©viter les cold starts)
- **Max Workers:** 3-5 (selon votre charge)
- **Idle Timeout:** 30-60 secondes

**Scaling :**
- **Scale Up Delay:** 5 secondes
- **Scale Down Delay:** 60 secondes

## ðŸ“Š Temps de DÃ©marrage

| ScÃ©nario | Temps |
|----------|-------|
| **Premier cold start** (avec prÃ©-cache) | ~2-3 min |
| **Cold start suivants** (cache /workspace) | ~15-30s |
| **Warm instance** | <5s |

## ðŸ” VÃ©rification

### Health Check
```bash
curl -X POST "https://api.runpod.ai/v2/<ENDPOINT_ID>/runsync" \
  -H "Authorization: Bearer $RUNPOD_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "input": { "task": "health" } }'
```

Vous devriez voir :
```json
{
  "output": {
    "ok": true,
    "info": {
      "transformers_has_voxtral": true,
      "cuda_available": true,
      ...
    }
  }
}
```

### VÃ©rifier le Cache
Dans les logs de votre pod :
```
[INIT] Loading processor...
[INIT] Processor loaded successfully  # <-- Pas de "Downloading" = cache utilisÃ©!
```

## ðŸ› Troubleshooting

### Build Ã©choue avec erreur HuggingFace
**ProblÃ¨me:** `401 Unauthorized` ou `403 Forbidden`
**Solution:** VÃ©rifiez que `HF_TOKEN` est bien configurÃ© et valide

### Cold start trop long (>5 min)
**ProblÃ¨me:** Les modÃ¨les ne sont pas prÃ©-cachÃ©s
**Solution:**
1. VÃ©rifiez que `HF_TOKEN` est dans les variables d'environnement RunPod
2. Regardez les logs du build pour confirmer le prÃ©-cache
3. Si le prÃ©-cache a Ã©chouÃ©, rebuild l'image

### ModÃ¨les tÃ©lÃ©chargÃ©s Ã  chaque appel
**ProblÃ¨me:** Le cache `/workspace` n'est pas persistant
**Solution:**
1. VÃ©rifiez que RunPod monte bien `/workspace`
2. Gardez au moins 1 worker minimum pour conserver une instance warm
3. Les instances diffÃ©rentes ont des caches diffÃ©rents (c'est normal)

### Erreur "VoxtralProcessor not found"
**ProblÃ¨me:** `mistral_common[audio]` n'est pas installÃ© correctement
**Solution:**
1. VÃ©rifiez que `requirements.txt` contient `mistral_common[audio]>=1.8.1`
2. Rebuild l'image depuis GitHub
3. VÃ©rifiez les logs du build pour voir si l'installation a rÃ©ussi

## ðŸ’¡ Astuces

1. **CoÃ»t optimisÃ©:** Utilisez min workers = 0 si vous acceptez les cold starts
2. **Performance max:** Utilisez min workers = 1 pour instance toujours warm
3. **Monitoring:** Surveillez les logs pour voir si le cache est utilisÃ©
4. **Updates:** Simplement push sur GitHub, RunPod rebuild automatiquement

## ðŸ“ž Support

- Documentation RunPod: https://docs.runpod.io/
- Documentation Voxtral: https://huggingface.co/mistralai/Voxtral-Small-24B-2507
- Issues GitHub: CrÃ©ez une issue sur votre repo
