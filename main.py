#!/usr/bin/env python3
"""
Voxtral Serverless Worker - Service de transcription avec diarisation
Optimis√© pour Runpod avec versions stables des transformers
"""

import os
import json
import time
import logging
import gc
import warnings
import signal
from contextlib import contextmanager
from typing import Optional, Dict, Any, List, Tuple
import tempfile
import requests
from pathlib import Path

import torch
import soundfile as sf
import librosa
import numpy as np
from transformers import AutoProcessor, AutoModelForCausalLM
from pyannote.audio import Pipeline
import runpod

# Configuration des warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message=".*torchcodec.*")
warnings.filterwarnings("ignore", message=".*libtorchcodec.*")

# Suppression des warnings torchcodec sp√©cifiquement
import logging
torchcodec_logger = logging.getLogger("pyannote.audio.core.io")
torchcodec_logger.setLevel(logging.ERROR)

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Configuration globale
VOXTRAL_MODEL = "mistralai/Voxtral-Small-24B-2507"
DIARIZATION_MODEL = "pyannote/speaker-diarization-3.1"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_DOWNLOAD_TIMEOUT = 3600  # 60 minutes pour le t√©l√©chargement (mod√®le 45GB)

@contextmanager
def timeout(duration):
    """Context manager pour timeout sur le t√©l√©chargement de mod√®les"""
    def timeout_handler(signum, frame):
        raise TimeoutError(f"Op√©ration timeout apr√®s {duration} secondes")
    
    # Configurer le signal d'alarme
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    
    try:
        yield
@contextmanager
def timeout(duration):
    """Context manager pour timeout sur le t√©l√©chargement de mod√®les"""
    def timeout_handler(signum, frame):
        raise TimeoutError(f"Op√©ration timeout apr√®s {duration} secondes")
    
    # Configurer le signal d'alarme
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(duration)
    
    try:
        yield
    finally:
        # Nettoyer
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

def warm_model_cache():
    """Pr√©-t√©l√©charge le mod√®le Voxtral pour acc√©l√©rer le d√©marrage"""
    from huggingface_hub import snapshot_download, scan_cache_dir
    
    logger.info("=== Voxtral Cache Warming ===")
    
    # V√©rifier le cache existant
    try:
        cache_info = scan_cache_dir()
        cached_repos = [repo.repo_id for repo in cache_info.repos]
        
        if VOXTRAL_MODEL in cached_repos:
            logger.info(f"‚úÖ Mod√®le {VOXTRAL_MODEL} d√©j√† en cache")
            for repo in cache_info.repos:
                if repo.repo_id == VOXTRAL_MODEL:
                    logger.info(f"   Taille: {repo.size_on_disk_str}")
            return True
        else:
            logger.info(f"üì• Mod√®le {VOXTRAL_MODEL} non trouv√© en cache, t√©l√©chargement...")
    except Exception as e:
        logger.warning(f"Impossible de v√©rifier le cache: {e}")
    
    # T√©l√©charger le mod√®le
    try:
        hf_token = os.getenv("HF_TOKEN")
        start_time = time.time()
        
        logger.info("üöÄ D√©but du t√©l√©chargement...")
        
        snapshot_path = snapshot_download(
            repo_id=VOXTRAL_MODEL,
            token=hf_token,
            cache_dir=None,
            resume_download=True,
            local_files_only=False
        )
        
        elapsed = time.time() - start_time
        logger.info(f"‚úÖ T√©l√©chargement termin√© en {elapsed:.1f}s")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du t√©l√©chargement: {e}")
        return False
MAX_DURATION = int(os.getenv("MAX_DURATION_S", "9000"))

# Variables globales pour le cache des mod√®les
voxtral_model = None
voxtral_processor = None
diarizer = None

def log_gpu_memory():
    """Affiche l'utilisation m√©moire GPU"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        free = total - allocated
        logger.info(f"[GPU] Total: {total:.1f}GB | Allocated: {allocated:.1f}GB | Cached: {cached:.1f}GB | Free: {free:.1f}GB")

def cleanup_gpu():
    """Nettoie la m√©moire GPU"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def load_voxtral_model() -> Tuple[Optional[Any], Optional[Any]]:
    """
    Charge le mod√®le Voxtral et son processor avec gestion d'erreurs robuste
    """
    global voxtral_model, voxtral_processor
    
    if voxtral_model is not None and voxtral_processor is not None:
        return voxtral_model, voxtral_processor
    
    try:
        logger.info(f"[VOXTRAL] Chargement du mod√®le: {VOXTRAL_MODEL}")
        
        # Chargement du processor avec gestion HF token
        hf_token = os.getenv("HF_TOKEN")
        # Chargement du processor Voxtral - approche simplifi√©e sans arguments probl√©matiques
        logger.info("[VOXTRAL] Chargement du processor...")
        
        try:
            # Approche directe sans passer par AutoProcessor qui ajoute des kwargs probl√©matiques
            from transformers.models.voxtral import VoxtralProcessor
            
            # Chargement sans les kwargs qui posent probl√®me
            voxtral_processor = VoxtralProcessor.from_pretrained(VOXTRAL_MODEL)
            logger.info("[VOXTRAL] ‚úì VoxtralProcessor charg√© directement")
            
        except Exception as e:
            logger.warning(f"[VOXTRAL] ‚ö† VoxtralProcessor direct √©chou√©: {e}")
            try:
                # Fallback: chargement manuel des composants
                from transformers import AutoTokenizer, AutoFeatureExtractor
                
                logger.info("[VOXTRAL] Chargement des composants individuels...")
                
                if hf_token:
                    tokenizer = AutoTokenizer.from_pretrained(VOXTRAL_MODEL, token=hf_token)
                    feature_extractor = AutoFeatureExtractor.from_pretrained(VOXTRAL_MODEL, token=hf_token)
                else:
                    tokenizer = AutoTokenizer.from_pretrained(VOXTRAL_MODEL)
                    feature_extractor = AutoFeatureExtractor.from_pretrained(VOXTRAL_MODEL)
                
                # Cr√©ation manuelle d'un objet processor
                class VoxtralProcessorManual:
                    def __init__(self, tokenizer, feature_extractor):
                        self.tokenizer = tokenizer
                        self.feature_extractor = feature_extractor
                    
                    def __call__(self, audio=None, text=None, sampling_rate=16000, return_tensors="pt"):
                        result = {}
                        if audio is not None:
                            # Process audio through feature extractor
                            audio_features = self.feature_extractor(
                                audio, 
                                sampling_rate=sampling_rate, 
                                return_tensors=return_tensors
                            )
                            result.update(audio_features)
                        if text is not None:
                            # Process text through tokenizer
                            text_features = self.tokenizer(
                                text, 
                                return_tensors=return_tensors,
                                padding=True,
                                truncation=True
                            )
                            result.update(text_features)
                        return result
                    
                    def batch_decode(self, *args, **kwargs):
                        return self.tokenizer.batch_decode(*args, **kwargs)
                
                voxtral_processor = VoxtralProcessorManual(tokenizer, feature_extractor)
                logger.info("[VOXTRAL] ‚úì Processor manuel cr√©√©")
                
            except Exception as e2:
                logger.error(f"[VOXTRAL] ‚úó Erreur processor manuel: {e2}")
                return None, None
        
        # Chargement du mod√®le avec approche simplifi√©e
        logger.info("[VOXTRAL] Chargement du mod√®le...")
        try:
            # V√©rification de la disponibilit√© du mod√®le dans le cache
            from huggingface_hub import scan_cache_dir, try_to_load_from_cache
            
            # Essayer de charger depuis le cache d'abord
            cache_info = None
            try:
                cache_info = scan_cache_dir()
                cached_repo = None
                for repo in cache_info.repos:
                    if VOXTRAL_MODEL.split('/')[-1] in repo.repo_id:
                        cached_repo = repo
                        break
                
                if cached_repo:
                    logger.info(f"[VOXTRAL] Mod√®le trouv√© dans le cache: {cached_repo.size_on_disk_str}")
                else:
                    logger.info("[VOXTRAL] Aucun cache trouv√©, t√©l√©chargement n√©cessaire")
            except Exception as cache_err:
                logger.warning(f"[VOXTRAL] ‚ö† Impossible de v√©rifier le cache: {cache_err}")
            
            # Chargement du vrai mod√®le Voxtral avec approche directe et timeout
            logger.info(f"[VOXTRAL] Chargement du mod√®le Voxtral (timeout: {MODEL_DOWNLOAD_TIMEOUT}s)...")
            
            try:
                # Premi√®re tentative: import direct de VoxtralForConditionalGeneration
                from transformers.models.voxtral import VoxtralForConditionalGeneration
                
                with timeout(MODEL_DOWNLOAD_TIMEOUT):
                    # Ajout de logs d√©taill√©s pour le suivi du t√©l√©chargement
                    logger.info("[VOXTRAL] D√©but du t√©l√©chargement/chargement du mod√®le (45GB)...")
                    start_time = time.time()
                    
                    if hf_token:
                        voxtral_model = VoxtralForConditionalGeneration.from_pretrained(
                            VOXTRAL_MODEL,
                            token=hf_token,
                            torch_dtype=torch.bfloat16,
                            device_map="auto",
                            low_cpu_mem_usage=True,
                            resume_download=True,
                            force_download=False,
                            local_files_only=False  # Permettre le t√©l√©chargement si n√©cessaire
                        )
                    else:
                        voxtral_model = VoxtralForConditionalGeneration.from_pretrained(
                            VOXTRAL_MODEL,
                            torch_dtype=torch.bfloat16,
                            device_map="auto",
                            low_cpu_mem_usage=True,
                            resume_download=True,
                            force_download=False,
                            local_files_only=False  # Permettre le t√©l√©chargement si n√©cessaire
                        )
                    
                    elapsed_time = time.time() - start_time
                    logger.info(f"[VOXTRAL] ‚úì Mod√®le charg√© en {elapsed_time:.1f}s")
                
                logger.info("[VOXTRAL] ‚úì VoxtralForConditionalGeneration charg√©")
                
            except (ImportError, TimeoutError) as e:
                if isinstance(e, TimeoutError):
                    logger.error(f"[VOXTRAL] ‚úó Timeout lors du chargement VoxtralForConditionalGeneration: {e}")
                else:
                    logger.warning("[VOXTRAL] ‚ö† VoxtralForConditionalGeneration non disponible, tentative AutoModel...")
                
                # Deuxi√®me tentative: utilisation d'AutoModel avec trust_remote_code et timeout
                try:
                    logger.info("[VOXTRAL] Tentative de fallback avec AutoModelForCausalLM...")
                    start_time_fallback = time.time()
                    
                    with timeout(MODEL_DOWNLOAD_TIMEOUT):
                        if hf_token:
                            voxtral_model = AutoModelForCausalLM.from_pretrained(
                                VOXTRAL_MODEL,
                                token=hf_token,
                                torch_dtype=torch.bfloat16,
                                device_map="auto",
                                trust_remote_code=True,
                                low_cpu_mem_usage=True,
                                resume_download=True,
                                force_download=False,
                                local_files_only=False
                            )
                        else:
                            voxtral_model = AutoModelForCausalLM.from_pretrained(
                                VOXTRAL_MODEL,
                                torch_dtype=torch.bfloat16,
                                device_map="auto",
                                trust_remote_code=True,
                                low_cpu_mem_usage=True,
                                resume_download=True,
                                force_download=False,
                                local_files_only=False
                            )
                    
                    elapsed_time_fallback = time.time() - start_time_fallback
                    logger.info(f"[VOXTRAL] ‚úì AutoModel charg√© en {elapsed_time_fallback:.1f}s")
                    
                    logger.info("[VOXTRAL] ‚úì AutoModelForCausalLM avec trust_remote_code charg√©")
                    
                except TimeoutError as timeout_err:
                    logger.error(f"[VOXTRAL] ‚úó Timeout lors du chargement AutoModel: {timeout_err}")
                    raise
            
            log_gpu_memory()
            return voxtral_model, voxtral_processor
            
        except Exception as e:
            logger.error(f"[VOXTRAL] ‚úó Erreur mod√®le: {e}")
            voxtral_processor = None
            return None, None
            
    except Exception as e:
        logger.error(f"[VOXTRAL] ‚úó Erreur g√©n√©rale: {e}")
        return None, None

def load_diarizer() -> Optional[Pipeline]:
    """
    Charge le pipeline de diarisation PyAnnote avec gestion des versions d'API
    """
    global diarizer
    
    if diarizer is not None:
        return diarizer
    
    try:
        logger.info(f"[DIARIZER] Chargement: {DIARIZATION_MODEL}")
        hf_token = os.getenv("HF_TOKEN")
        
        try:
            # Premi√®re tentative avec token (nouvelle API)
            if hf_token:
                diarizer = Pipeline.from_pretrained(
                    DIARIZATION_MODEL,
                    token=hf_token
                )
            else:
                diarizer = Pipeline.from_pretrained(DIARIZATION_MODEL)
        except Exception as e1:
            logger.warning(f"[DIARIZER] ‚ö† token √©chou√©: {e1}")
            try:
                # Deuxi√®me tentative avec use_auth_token (ancienne API)
                if hf_token:
                    diarizer = Pipeline.from_pretrained(
                        DIARIZATION_MODEL,
                        use_auth_token=hf_token
                    )
                else:
                    diarizer = Pipeline.from_pretrained(DIARIZATION_MODEL)
            except Exception as e2:
                logger.warning(f"[DIARIZER] ‚ö† use_auth_token √©chou√©: {e2}")
                # Troisi√®me tentative sans authentification
                diarizer = Pipeline.from_pretrained(DIARIZATION_MODEL)
            
        diarizer.to(torch.device(DEVICE))
        logger.info("[DIARIZER] ‚úì Diarizer charg√© avec succ√®s")
        return diarizer
        
    except Exception as e:
        logger.error(f"[DIARIZER] ‚úó Erreur: {e}")
        return None

def transcribe_with_voxtral(audio_path: str, max_tokens: int = 2500) -> str:
    """
    Transcrit un fichier audio avec Voxtral
    """
    try:
        logger.info(f"[VOXTRAL] D√©but transcription: {audio_path}")
        model, processor = load_voxtral_model()
        if model is None or processor is None:
            raise Exception("Impossible de charger Voxtral")
        
        logger.info(f"[VOXTRAL] Transcription (max_tokens={max_tokens})")
        
        # Chargement de l'audio
        logger.info("[VOXTRAL] Lecture du fichier audio...")
        audio, sample_rate = sf.read(audio_path)
        logger.info(f"[VOXTRAL] Audio: {len(audio)} samples @ {sample_rate}Hz")
        
        if len(audio.shape) > 1:
            audio = audio.mean(axis=1)
            logger.info("[VOXTRAL] Audio converti en mono")
        
        # Conversion au sample rate attendu (16kHz g√©n√©ralement)
        if sample_rate != 16000:
            logger.info(f"[VOXTRAL] Resampling {sample_rate}Hz -> 16000Hz")
            audio = librosa.resample(audio, orig_sr=sample_rate, target_sr=16000)
            sample_rate = 16000
        
        # Pr√©paration des inputs avec le processor
        logger.info("[VOXTRAL] Pr√©paration des inputs...")
        inputs = processor(
            audio=audio,
            sampling_rate=sample_rate,
            return_tensors="pt"
        )
        logger.info(f"[VOXTRAL] Inputs pr√©par√©s: {list(inputs.keys())}")
        
        # D√©placement vers GPU si disponible
        if DEVICE == "cuda":
            logger.info("[VOXTRAL] D√©placement vers GPU...")
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        # G√©n√©ration avec Voxtral
        logger.info("[VOXTRAL] G√©n√©ration en cours...")
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=False,
                temperature=0.0,
                pad_token_id=processor.tokenizer.eos_token_id if hasattr(processor, 'tokenizer') else 2
            )
        
        logger.info("[VOXTRAL] D√©codage...")
        # D√©codage de la transcription
        transcription = processor.batch_decode(
            generated_ids,
            skip_special_tokens=True
        )[0]
        
        logger.info(f"[VOXTRAL] ‚úì Transcription termin√©e ({len(transcription)} caract√®res)")
        logger.info(f"[VOXTRAL] Aper√ßu: {transcription[:100]}...")
        return transcription.strip()
        
    except Exception as e:
        logger.error(f"[VOXTRAL] ‚úó Erreur transcription: {e}")
        import traceback
        traceback.print_exc()
        return ""

def perform_diarization(audio_path: str) -> List[Dict]:
    """
    Effectue la diarisation sur un fichier audio
    """
    try:
        diarizer_pipeline = load_diarizer()
        if diarizer_pipeline is None:
            raise Exception("Impossible de charger le diarizer")
        
        logger.info("[DIARIZER] Analyse de la diarisation...")
        diarization = diarizer_pipeline(audio_path)
        
        segments = []
        for turn, _, speaker in diarization.itertracks(yield_label=True):
            segments.append({
                "start": round(turn.start, 2),
                "end": round(turn.end, 2),
                "speaker": speaker,
                "duration": round(turn.end - turn.start, 2)
            })
        
        logger.info(f"[DIARIZER] ‚úì {len(segments)} segments d√©tect√©s")
        return segments
        
    except Exception as e:
        logger.error(f"[DIARIZER] ‚úó Erreur: {e}")
        return []

def extract_audio_segment(audio_path: str, start_time: float, end_time: float) -> str:
    """
    Extrait un segment audio entre start_time et end_time
    """
    try:
        # Lecture de l'audio complet
        audio, sample_rate = sf.read(audio_path)
        
        # Calcul des indices de d√©but et fin
        start_idx = int(start_time * sample_rate)
        end_idx = int(end_time * sample_rate)
        
        # Extraction du segment
        segment = audio[start_idx:end_idx]
        
        # Sauvegarde temporaire
        temp_path = f"/tmp/segment_{int(start_time)}_{int(end_time)}.wav"
        sf.write(temp_path, segment, sample_rate)
        
        return temp_path
        
    except Exception as e:
        logger.error(f"[EXTRACT] Erreur extraction segment: {e}")
        return ""

def analyze_sentiment(text: str) -> str:
    """
    Analyse basique du sentiment (peut √™tre √©tendue avec un mod√®le d√©di√©)
    """
    if not text:
        return "neutre"
    
    # Mots-cl√©s positifs et n√©gatifs (fran√ßais)
    positive_words = ["merci", "bien", "parfait", "excellent", "content", "satisfait", "ok"]
    negative_words = ["probl√®me", "erreur", "mal", "mauvais", "pas bien", "insatisfait", "d√©√ßu"]
    
    text_lower = text.lower()
    positive_count = sum(1 for word in positive_words if word in text_lower)
    negative_count = sum(1 for word in negative_words if word in text_lower)
    
    if positive_count > negative_count:
        return "positif"
    elif negative_count > positive_count:
        return "n√©gatif"
    else:
        return "neutre"

def create_summary(transcription: str) -> str:
    """
    Cr√©e un r√©sum√© basique de la transcription
    """
    if not transcription:
        return "Aucune transcription disponible"
    
    # R√©sum√© simple par troncature intelligente
    sentences = transcription.split('.')
    if len(sentences) <= 3:
        return transcription
    
    # Prendre les premi√®res et derni√®res phrases
    summary_sentences = sentences[:2] + sentences[-1:]
    summary = '. '.join(s.strip() for s in summary_sentences if s.strip())
    
    return f"R√©sum√©: {summary}"

def download_audio(url: str) -> str:
    """
    T√©l√©charge un fichier audio depuis une URL
    """
    try:
        logger.info(f"[DOWNLOAD] T√©l√©chargement: {url}")
        response = requests.get(url, timeout=60)
        response.raise_for_status()
        
        # Cr√©ation d'un fichier temporaire
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as temp_file:
            temp_file.write(response.content)
            temp_path = temp_file.name
        
        logger.info(f"[DOWNLOAD] ‚úì Audio t√©l√©charg√©: {temp_path}")
        return temp_path
        
    except Exception as e:
        logger.error(f"[DOWNLOAD] ‚úó Erreur: {e}")
        return ""

def process_audio_request(job_input: Dict[str, Any]) -> Dict[str, Any]:
    """
    Traite une demande de transcription audio
    """
    temp_files = []
    
    try:
        # Extraction des param√®tres
        task = job_input.get("task", "transcribe")
        audio_url = job_input.get("audio_url", "")
        language = job_input.get("language", "fr")
        max_tokens = job_input.get("max_tokens", 2500)
        include_summary = job_input.get("summary", False)
        include_sentiment = job_input.get("sentiment", False)
        
        logger.info(f"[HANDLER] T√¢che: {task}, langue: {language}, tokens: {max_tokens}")
        
        if not audio_url:
            return {"error": "URL audio manquante"}
        
        # T√©l√©chargement de l'audio
        audio_path = download_audio(audio_url)
        if not audio_path:
            return {"error": "√âchec du t√©l√©chargement audio"}
        temp_files.append(audio_path)
        
        # V√©rification de la dur√©e
        audio, sr = sf.read(audio_path)
        duration = len(audio) / sr
        logger.info(f"[HANDLER] Dur√©e audio: {duration:.1f}s")
        
        if duration > MAX_DURATION:
            return {"error": f"Audio trop long (max {MAX_DURATION}s)"}
        
        result = {"task": task, "language": language, "duration": round(duration, 2)}
        
        # Traitement selon la t√¢che demand√©e
        if task in ["transcribe", "transcribe_diarized"]:
            if task == "transcribe_diarized":
                # Diarisation + transcription par segment
                logger.info("[HANDLER] Mode diarisation activ√©")
                segments = perform_diarization(audio_path)
                
                if segments:
                    transcriptions = []
                    for segment in segments:
                        # Extraction du segment audio
                        segment_path = extract_audio_segment(
                            audio_path, segment["start"], segment["end"]
                        )
                        if segment_path:
                            temp_files.append(segment_path)
                            
                            # Transcription du segment
                            segment_text = transcribe_with_voxtral(segment_path, 500)
                            
                            transcriptions.append({
                                "speaker": segment["speaker"],
                                "start": segment["start"],
                                "end": segment["end"],
                                "text": segment_text
                            })
                    
                    result["transcriptions"] = transcriptions
                    
                    # Transcription compl√®te concaten√©e
                    full_transcription = " ".join(t["text"] for t in transcriptions if t["text"])
                else:
                    # Fallback: transcription sans diarisation
                    logger.info("[HANDLER] Fallback: transcription simple")
                    full_transcription = transcribe_with_voxtral(audio_path, max_tokens)
                    result["transcriptions"] = [{"speaker": "UNKNOWN", "text": full_transcription}]
            else:
                # Transcription simple
                full_transcription = transcribe_with_voxtral(audio_path, max_tokens)
                result["transcription"] = full_transcription
            
            # Ajout du r√©sum√© si demand√©
            if include_summary and 'transcriptions' in result:
                full_text = " ".join(t["text"] for t in result["transcriptions"] if t["text"])
                result["summary"] = create_summary(full_text)
            elif include_summary and 'transcription' in result:
                result["summary"] = create_summary(result["transcription"])
            
            # Ajout du sentiment si demand√©
            if include_sentiment and 'transcriptions' in result:
                full_text = " ".join(t["text"] for t in result["transcriptions"] if t["text"])
                result["sentiment"] = analyze_sentiment(full_text)
            elif include_sentiment and 'transcription' in result:
                result["sentiment"] = analyze_sentiment(result["transcription"])
        
        logger.info("[HANDLER] ‚úì Traitement termin√© avec succ√®s")
        return result
        
    except Exception as e:
        logger.error(f"[HANDLER] ‚úó Erreur: {e}")
        return {"error": str(e)}
        
    finally:
        # Nettoyage des fichiers temporaires
        for temp_file in temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception:
                pass
        
        cleanup_gpu()

def initialize_models():
    """
    Initialise les mod√®les au d√©marrage (optionnel)
    """
    try:
        logger.info("[INIT] Pr√©-chargement des mod√®les...")
        log_gpu_memory()
        
        # Pr√©-chargement Voxtral
        model, processor = load_voxtral_model()
        if model and processor:
            logger.info("[INIT] ‚úì Voxtral pr√©-charg√©")
        else:
            logger.warning("[INIT] ‚ö† √âchec pr√©-chargement Voxtral")
        
        # Pr√©-chargement diarizer
        diarizer_pipeline = load_diarizer()
        if diarizer_pipeline:
            logger.info("[INIT] ‚úì Diarizer pr√©-charg√©")
        else:
            logger.warning("[INIT] ‚ö† √âchec pr√©-chargement Diarizer")
        
        log_gpu_memory()
        logger.info("[INIT] ‚úì Initialisation termin√©e")
        
    except Exception as e:
        logger.error(f"[INIT] ‚úó Erreur initialisation: {e}")

# Point d'entr√©e principal pour RunPod
def handler(job):
    """
    Handler principal pour RunPod serverless
    """
    try:
        logger.info(f"[HANDLER] Nouveau job re√ßu: {job}")
        job_input = job.get("input", {})
        
        if not job_input:
            logger.error("[HANDLER] ‚úó Pas d'input dans le job")
            return {"error": "Pas d'input fourni"}
        
        result = process_audio_request(job_input)
        logger.info(f"[HANDLER] ‚úì Job termin√©. R√©sultat: {len(str(result))} caract√®res")
        return result
        
    except Exception as e:
        logger.error(f"[HANDLER] ‚úó Erreur dans handler: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"Erreur handler: {str(e)}"}

if __name__ == "__main__":
    import sys
    
    # Support pour cache warming
    if len(sys.argv) > 1 and sys.argv[1] == "warm_cache":
        success = warm_model_cache()
        sys.exit(0 if success else 1)
    
    # Test local ou d√©marrage RunPod
    if os.getenv("RUNPOD_DEBUG"):
        # Mode debug local
        test_input = {
            "task": "transcribe_diarized",
            "audio_url": "https://example.com/test.wav",
            "language": "fr",
            "max_tokens": 1000,
            "summary": True,
            "sentiment": True
        }
        result = process_audio_request(test_input)
        print(json.dumps(result, indent=2, ensure_ascii=False))
    else:
        # Mode production RunPod
        initialize_models()
        runpod.serverless.start({"handler": handler})
